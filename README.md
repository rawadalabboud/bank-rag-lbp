# Assistant LBP (RAG) ‚Äî Chatbot bancaire FR üá´üá∑üí≥

_Un assistant bancaire bas√© sur **Retrieval-Augmented Generation (RAG)** pour r√©pondre en fran√ßais aux questions clients √† partir des pages officielles de **La Banque Postale**, **Service-Public.fr**, et **Banque de France**._

> ‚úÖ R√©ponses **sourc√©es** (titre + URL officiel)  
> ‚úÖ Mode **extraction factuelle** (sans LLM) ou **g√©n√©ratif** (Hugging Face / OpenAI / Ollama)  
> ‚úÖ √âvaluations automatiques (retrieval & g√©n√©ration) sur un **jeu YAML de 63 cas m√©tier**  
> ‚úÖ Petite app web (FastAPI + HTML/CSS) avec logo **La Banque Postale**

---

## Sommaire

- [Aper√ßu](#aper√ßu)
- [Architecture](#architecture)
- [Arborescence](#arborescence)
- [Installation](#installation)
- [Ingestion des donn√©es](#ingestion-des-donn√©es)
- [Recherche & R√©ponse](#recherche--r√©ponse)
- [Application Web](#application-web)
- [√âvaluation](#√©valuation)
- [Configuration](#configuration)
- [Ajouter des sources](#ajouter-des-sources)
- [D√©pannage](#d√©pannage)
- [Licence & mentions](#licence--mentions)

---

## Aper√ßu

Le pipeline repose sur trois √©tapes principales :

1. **R√©cup√©ration** des passages pertinents via embeddings + FAISS.  
2. **Compression** du contexte (scoring par phrase) pour rester efficace et √©viter le bruit.  
3. **G√©n√©ration contr√¥l√©e** :  
   - Mode **LLM** (HF, OpenAI, Ollama)  
   - Mode **fallback extractif** (sans LLM, renvoie directement les phrases pertinentes)  

Chaque r√©ponse cite toujours les **sources officielles**.

---

## Architecture

```
[Sources YAML/Markdown] -> ingest.py -> [FAISS index]
                                      |
query.py: Question -> Retrieve -> Boost (optionnel) -> Context Compress
         -> (LLM ou Extractif) -> R√©ponse + Sources
```

- **Corpus** : pages HTML/PDF converties en Markdown avec front-matter  
- **Embeddings** : Hugging Face (`BAAI/bge-m3`) par d√©faut  
- **Vector store** : FAISS  
- **LLM** : Hugging Face Inference (Meta Llama 3.1 / Qwen2.5) ou OpenAI GPT  

---

## Arborescence

```
bank-rag-fr-corpus/
‚îú‚îÄ app/                     # FastAPI UI
‚îÇ  ‚îî‚îÄ main.py
‚îú‚îÄ data/                    # Markdown (corpus)
‚îú‚îÄ faiss_index/             # Index FAISS (g√©n√©r√©)
‚îú‚îÄ scripts/
‚îÇ  ‚îú‚îÄ ingest.py             # Split + embeddings + FAISS
‚îÇ  ‚îú‚îÄ query.py              # Pipeline RAG
‚îÇ  ‚îú‚îÄ eval_bank_fr.yaml     # Jeu de 63 cas m√©tier (tarifs, virements, s√©curit√©‚Ä¶)
‚îÇ  ‚îú‚îÄ eval_retrieval.py     # √âval. retrieval (recall@k, mrr@k)
‚îÇ  ‚îú‚îÄ eval_rag.py           # √âval. RAG (fact_em, grounded_ok, refusals‚Ä¶)
‚îÇ  ‚îî‚îÄ fetch_to_md.py        # (optionnel) conversion HTML/PDF -> MD
‚îú‚îÄ .env.prod                # Config (sans secrets)
‚îú‚îÄ requirements.txt
‚îî‚îÄ README.md
```

---

## Installation

```bash
python3 -m venv .venv
source .venv/bin/activate
pip install -U pip
pip install -r requirements.txt
```

Configurer l‚Äôenvironnement :

```bash
cp .env.prod .env   # version de prod, sans secrets
```

---

## Ingestion des donn√©es

1. **Pr√©parer le corpus** (`data/*.md`) :  
   - soit g√©n√©r√© via `fetch_to_md.py`  
   - soit copi√© depuis le d√©p√¥t (LBP, Banque de France, Service-Public)  

2. **Cr√©er l‚Äôindex FAISS** :  

```bash
HF_EMBED_MODEL="BAAI/bge-m3" VECTORSTORE=faiss python3 scripts/ingest.py
```

---

## Recherche & R√©ponse

### Mode extraction (sans LLM)

```bash
LLM_BACKEND=none python3 scripts/query.py "Quels sont les frais d‚Äôun virement SEPA ?"
```

‚Üí renvoie les phrases les plus pertinentes + URL source.

### Mode LLM (Hugging Face)

```bash
LLM_BACKEND=hf HF_REPO_ID="meta-llama/Meta-Llama-3.1-70B-Instruct" TOP_K=5 COMP_CHARS=1000 python3 scripts/query.py "Comment activer Certicode Plus ?"
```

### Mode OpenAI

```bash
LLM_BACKEND=openai CHAT_MODEL=gpt-4o-mini OPENAI_API_KEY=sk-... python3 scripts/query.py "Quels sont les plafonds d‚Äôune Visa Premier ?"
```

---

## Application Web

```bash
uvicorn app.main:app --reload --port 8000
```

- Interface simple type **chat**  
- R√©ponses sourc√©es, citations cliquables  
- Logo LBP dans `app/static/`

---

## √âvaluation

### 1. √âvaluer le **retrieval seul**

```bash
HF_EMBED_MODEL="BAAI/bge-m3" python3 scripts/eval_retrieval.py
```

‚Üí calcule `recall@5`, `recall@10`, `mrr@10`

### 2. √âvaluer le **RAG complet**

```bash
LLM_BACKEND=hf HF_REPO_ID="meta-llama/Meta-Llama-3.1-70B-Instruct" TOP_K=5 COMP_CHARS=1100 python3 scripts/eval_rag.py
```

‚Üí calcule :
- **Retrieval** : recall/mrr  
- **G√©n√©ration** : grounded_ok, citation_precision, refusals  
- **fact_em** : activable si regex fournis dans YAML (par d√©faut d√©sactiv√© en prod)

R√©sultats sauvegard√©s dans :
- `eval_results.csv` (par item)
- `eval_summary.json` (moyennes)

---

## Configuration

Variable | R√¥le
---|---
`HF_EMBED_MODEL` | mod√®le d‚Äôembeddings (par d√©faut `BAAI/bge-m3`)
`VECTORSTORE` | FAISS uniquement
`LLM_BACKEND` | none, hf, openai, ollama
`HF_REPO_ID` | mod√®le HF (ex: llama-3.1-70B-instruct)
`CHAT_MODEL` | mod√®le OpenAI (ex: gpt-4o-mini)
`TOP_K` | nb docs √† r√©cup√©rer
`COMP_CHARS` | budget contexte compress√©
`AMOUNT_BOOST` | `on/off` boost des PDF tarifs
`FACT_EM_MODE` | `on/off` validation regex factuelles

---

## Ajouter des sources

1. Ajouter l‚ÄôURL dans `sources_fr.yaml`  
2. `python3 scripts/fetch_to_md.py`  
3. `python3 scripts/ingest.py`  

---

## D√©pannage

- **LibreSSL warning (macOS)** ‚Üí sans impact  
- **R√©ponses trop longues** ‚Üí baisser `COMP_CHARS` ou `HF_MAX_NEW_TOKENS`  
- **Pas de LLM dispo** ‚Üí `LLM_BACKEND=none` pour extraction factuelle  
- **fact_em trop bas** ‚Üí ajuster ou d√©sactiver (`FACT_EM_MODE=off`)  

---

## Licence & mentions

- Code : **MIT**  
- Corpus : **La Banque Postale**, **Service-Public**, **Banque de France**  
- Logo **LBP** : utilis√© uniquement pour la d√©monstration (marque d√©pos√©e)
